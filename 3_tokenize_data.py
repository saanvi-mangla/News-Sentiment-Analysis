# -*- coding: utf-8 -*-
"""3_tokenize_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oD6C5RVOUk2ljkMec2UK15YybJ1xzOs3
"""



# -*- coding: utf-8 -*-
import pandas as pd
from datasets import load_dataset, ClassLabel
from transformers import AutoTokenizer
import logging
import os
import config

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s : %(levelname)s : %(message)s')

def tokenize_function(examples, tokenizer):
    """Tokenizes a batch of text examples."""
    text_batch = [str(t) if t is not None else "" for t in examples[config.TEXT_COLUMN]]
    return tokenizer(text_batch, padding="max_length", truncation=True, max_length=256)

def main():
    """Loads split data, tokenizes it, and saves the tokenized dataset to disk."""
    print("--- Starting Tokenization Script ---")

    # Check for input files
    split_files = [config.TRAIN_SET_CSV, config.VALIDATION_SET_CSV, config.TEST_SET_CSV]
    if not all(os.path.exists(f) for f in split_files):
        raise FileNotFoundError("Split CSV files not found. Run 2_split_data.py first.")

    # Load data from CSVs
    logging.info("Loading data splits for tokenization...")
    raw_datasets = load_dataset('csv', data_files={
        'train': config.TRAIN_SET_CSV,
        'validation': config.VALIDATION_SET_CSV,
        'test': config.TEST_SET_CSV
    })
    print(raw_datasets)

    # Prepare labels
    logging.info("Preparing labels for tokenization...")
    unique_labels = sorted(raw_datasets['train'].unique(config.LABEL_COLUMN))
    class_label_feature = ClassLabel(names=unique_labels)
    id2label = {id: label for id, label in enumerate(class_label_feature.names)}
    label2id = {label: id for id, label in id2label.items()}
    logging.info(f"Label mapping created: {label2id}")

    def map_labels_to_ids(batch):
        batch['labels'] = [label2id.get(str(lbl).strip().capitalize(), -1) for lbl in batch[config.LABEL_COLUMN]]
        return batch

    raw_datasets = raw_datasets.map(map_labels_to_ids, batched=True)

    new_features = raw_datasets['train'].features.copy()
    new_features['labels'] = class_label_feature
    raw_datasets = raw_datasets.cast(new_features)

    logging.info("Dataset features after label mapping:")
    print(raw_datasets['train'].features)

    # Load tokenizer
    logging.info(f"Loading tokenizer for model: {config.MODEL_CHECKPOINT}")
    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_CHECKPOINT)

    # Apply tokenization
    logging.info(f"Tokenizing text from column: '{config.TEXT_COLUMN}'")
    cols_to_remove = raw_datasets['train'].column_names
    cols_to_remove.remove('labels') # Keep the 'labels' column

    tokenized_datasets = raw_datasets.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True,
        remove_columns=cols_to_remove
    )

    tokenized_datasets.set_format("torch")
    logging.info("Dataset format set for PyTorch.")

    # Save processed data to disk
    logging.info(f"Saving tokenized dataset to: {config.TOKENIZED_DATASET_PATH}")
    tokenized_datasets.save_to_disk(config.TOKENIZED_DATASET_PATH)

    print("\n--- SUCCESS: Data is tokenized and saved to disk! ---")
    print(f"--- The tokenized data is now ready in the '{config.TOKENIZED_DATASET_PATH}' directory. ---")

if __name__ == '__main__':
    main()
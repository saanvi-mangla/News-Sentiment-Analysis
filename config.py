# -*- coding: utf-8 -*-
"""config.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oD6C5RVOUk2ljkMec2UK15YybJ1xzOs3
"""



import torch

TEXT_COLUMN = 'Headline'
LABEL_COLUMN = 'Sentiment'

START_URL = 'https://www.punjabitribuneonline.com/news/nation/'
SOURCE_WEBSITE = 'Punjabi Tribune'
SCRAPED_DATA_CSV = 'punjabi_tribune_nation_selenium_updated.csv'
MAX_ARTICLES_PER_RUN = 5000
ARTICLE_SCRAPE_DELAY = 3
LOAD_MORE_DELAY = 5
MAX_LOAD_MORE_ATTEMPTS = 1000
CHROME_BINARY_PATH = "/usr/bin/google-chrome"

LABELED_DATA_CSV = 'punjabi_news_sentiment_labelled_dataset_finall_3june.csv'
TRAIN_SET_CSV = 'train_set.csv'
VALIDATION_SET_CSV = 'validation_set.csv'
TEST_SET_CSV = 'test_set.csv'
TEST_SET_SIZE = 0.10
VALIDATION_SET_SIZE = 0.10

MODEL_CHECKPOINT = "xlm-roberta-base"
TOKENIZED_DATASET_PATH = "tokenized_dataset"
FINAL_MODEL_DIR = "punjabi-news-sentiment-xlm-roberta_final"

LLM_PROVIDER = "GoogleGemini"
GOOGLE_API_KEY = "PASTE_YOUR_GOOGLE_API_KEY_HERE"
GEMINI_MODEL_FOR_SUMMARY = "gemini-1.5-flash-latest"
GEMINI_MODEL_FOR_QNA = "gemini-1.5-flash-latest"

def get_device():
    if torch.cuda.is_available():
        print("GPU available. Using CUDA.")
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        print("MPS available (Apple Silicon). Using MPS.")
        return torch.device("mps")
    else:
        print("No GPU or MPS available. Using CPU.")
        return torch.device("cpu")

DEVICE = get_device()
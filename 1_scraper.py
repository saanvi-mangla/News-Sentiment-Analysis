# -*- coding: utf-8 -*-
"""1_scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oD6C5RVOUk2ljkMec2UK15YybJ1xzOs3
"""



# -*- coding: utf-8 -*-
import csv
import requests
from bs4 import BeautifulSoup
import time
import logging
from urllib.parse import urljoin
import tempfile
import os
import shutil
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from webdriver_manager.chrome import ChromeDriverManager
import config # Import the configuration file

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- CSS Selectors ---
load_more_button_selector = ".load-button"
article_container_selector = "div.listing"
article_link_selector = 'h2.is-title.post-title a'
headline_selector = 'h1.is-title.post-title'
article_text_container_selector = 'div.entry-content'
date_selector = 'time.post-date'

# --- User Agent ---
requests_headers = {
    'User-Agent': 'MyPoliticalNewsScraper/1.0+Selenium (Contact: your-email@example.com)'
}

# --- Header for CSV ---
csv_header = ['Headline', 'Article Text', 'URL', 'Source', 'Publication Date']

def scrape_article(article_url):
    """Fetches and scrapes headline, text, and date from a single article URL using Requests."""
    try:
        logging.info(f"[Requests] Requesting article: {article_url}")
        response = requests.get(article_url, headers=requests_headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        headline = soup.select_one(headline_selector).get_text(strip=True) if soup.select_one(headline_selector) else "Headline not found"
        text_container = soup.select_one(article_text_container_selector)
        article_text = '\n'.join([p.get_text(strip=True) for p in text_container.find_all('p')]) if text_container else "Article text not found"
        publication_date = soup.select_one(date_selector).get_text(strip=True) if soup.select_one(date_selector) else "Date not found"

        if headline == "Headline not found" and article_text == "Article text not found":
            logging.warning(f"[Requests] Could not extract headline or body from {article_url}")
            return None

        logging.info(f"[Requests] Scraped: Headline='{headline[:30]}...', Date='{publication_date}'")
        return {
            'Headline': headline, 'Article Text': article_text, 'URL': article_url,
            'Source': config.SOURCE_WEBSITE, 'Publication Date': publication_date
        }
    except requests.exceptions.RequestException as e:
        logging.error(f"[Requests] Network error scraping {article_url}: {e}")
    except Exception as e:
        logging.error(f"[Requests] Error parsing {article_url}: {e}")
    return None

def read_existing_urls(csv_file):
    """Reads already scraped URLs from the output CSV to prevent duplicates."""
    if not os.path.exists(csv_file):
        logging.info(f"Output file {csv_file} not found. This will be the first run.")
        return set(), False

    existing_urls = set()
    try:
        with open(csv_file, 'r', newline='', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row.get('URL'):
                    existing_urls.add(row['URL'])
        logging.info(f"Found {len(existing_urls)} existing URLs in {csv_file}.")
        return existing_urls, True
    except Exception as e:
        logging.error(f"Error reading {csv_file}: {e}. Starting fresh check.")
        return set(), True # File exists but couldn't be read

def append_to_csv(data_list, csv_file, file_existed):
    """Appends a list of dictionaries to a CSV file."""
    if not data_list:
        logging.info("No new articles to append.")
        return

    is_empty = not file_existed or os.path.getsize(csv_file) == 0
    try:
        with open(csv_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=csv_header)
            if is_empty:
                writer.writeheader()
                logging.info("Writing CSV header.")
            writer.writerows(data_list)
        logging.info(f"SUCCESS: Appended {len(data_list)} new articles to {csv_file}")
    except IOError as e:
        logging.error(f"Error appending to CSV file {csv_file}: {e}")

def main():
    """Main scraping function."""
    existing_urls, file_existed = read_existing_urls(config.SCRAPED_DATA_CSV)
    driver = None
    all_article_urls = set()
    user_data_dir = tempfile.mkdtemp(prefix="selenium_chrome_user_data_")

    try:
        logging.info("Setting up Chrome WebDriver...")
        service = Service(ChromeDriverManager().install())
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument(f'user-agent={requests_headers["User-Agent"]}')
        options.add_argument(f"--user-data-dir={user_data_dir}")
        if config.CHROME_BINARY_PATH and os.path.exists(config.CHROME_BINARY_PATH):
            options.binary_location = config.CHROME_BINARY_PATH

        driver = webdriver.Chrome(service=service, options=options)
        logging.info(f"Loading initial page: {config.START_URL}")
        driver.get(config.START_URL)
        time.sleep(5)

        for attempt in range(config.MAX_LOAD_MORE_ATTEMPTS):
            try:
                logging.info(f"Load More Attempt {attempt + 1}/{config.MAX_LOAD_MORE_ATTEMPTS}")
                wait = WebDriverWait(driver, 20)
                load_more_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, load_more_button_selector)))
                driver.execute_script("arguments[0].click();", load_more_button)
                time.sleep(config.LOAD_MORE_DELAY)
            except TimeoutException:
                logging.info("'Load More' button not found. Assuming all content is loaded.")
                break
            except Exception as e:
                logging.error(f"Error clicking 'Load More': {e}", exc_info=True)
                break

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        link_elements = soup.select(f"{article_container_selector} {article_link_selector}")
        logging.info(f"Found {len(link_elements)} potential article links.")

        for link in link_elements:
            if 'href' in link.attrs:
                all_article_urls.add(urljoin(config.START_URL, link['href']))

        logging.info(f"Collected {len(all_article_urls)} unique URLs.")

    except Exception as e:
        logging.error(f"An error occurred during Selenium phase: {e}", exc_info=True)
    finally:
        if driver:
            driver.quit()
            shutil.rmtree(user_data_dir)
            logging.info("WebDriver closed and temp directory cleaned up.")

    # Filter out existing URLs and scrape new ones
    urls_to_scrape = [url for url in all_article_urls if url not in existing_urls]
    logging.info(f"Found {len(urls_to_scrape)} new articles to scrape.")

    newly_scraped_data = []
    for i, url in enumerate(urls_to_scrape):
        if len(newly_scraped_data) >= config.MAX_ARTICLES_PER_RUN:
            logging.info(f"Reached max articles limit ({config.MAX_ARTICLES_PER_RUN}). Stopping.")
            break

        logging.info(f"Processing NEW Article {i+1}/{len(urls_to_scrape)}")
        article_data = scrape_article(url)
        if article_data:
            newly_scraped_data.append(article_data)

        time.sleep(config.ARTICLE_SCRAPE_DELAY)

    append_to_csv(newly_scraped_data, config.SCRAPED_DATA_CSV, file_existed)
    logging.info("--- Scraping script finished. ---")


if __name__ == "__main__":
    main()
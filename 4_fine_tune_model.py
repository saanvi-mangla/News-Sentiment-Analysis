# -*- coding: utf-8 -*-
"""4_fine_tune_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oD6C5RVOUk2ljkMec2UK15YybJ1xzOs3
"""



# -*- coding: utf-8 -*-
import torch
import numpy as np
from sklearn.metrics import accuracy_score, f1_score
from sklearn.utils.class_weight import compute_class_weight
from datasets import load_from_disk, ClassLabel
from transformers import (
    AutoModelForSequenceClassification, AutoTokenizer,
    TrainingArguments, Trainer, DataCollatorWithPadding
)
import logging
import config

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s : %(levelname)s : %(message)s')

def compute_metrics(eval_pred):
    """Computes accuracy and F1 score for evaluation."""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="weighted", zero_division=0)
    return {"accuracy": acc, "f1": f1}

class CustomTrainer(Trainer):
    """A custom trainer to handle class weights for imbalanced datasets."""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Calculate class weights
        train_labels = self.train_dataset["labels"]
        class_weights = compute_class_weight(
            class_weight='balanced',
            classes=np.unique(train_labels),
            y=train_labels
        )
        self.class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.args.device)
        logging.info(f"Using custom trainer with class weights: {self.class_weights.tolist()}")

    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

def main():
    """Loads tokenized data, fine-tunes the model, and saves the final artifact."""
    print("--- Starting Model Fine-Tuning Script ---")

    # Load tokenized dataset from disk
    logging.info(f"Loading tokenized dataset from {config.TOKENIZED_DATASET_PATH}...")
    tokenized_datasets = load_from_disk(config.TOKENIZED_DATASET_PATH)
    logging.info("Dataset loaded successfully.")

    # Get label mappings from the dataset features
    label_feature = tokenized_datasets['train'].features['labels']
    id2label = {id: label for id, label in enumerate(label_feature.names)}
    label2id = {label: id for id, label in id2label.items()}
    num_labels = label_feature.num_classes
    logging.info(f"Loaded label mapping from dataset: {id2label}")

    # Load pre-trained model
    logging.info(f"Loading pre-trained model: {config.MODEL_CHECKPOINT}")
    model = AutoModelForSequenceClassification.from_pretrained(
        config.MODEL_CHECKPOINT, num_labels=num_labels, id2label=id2label,
        label2id=label2id, ignore_mismatched_sizes=True
    ).to(config.DEVICE)

    # Define training arguments
    logging.info("Defining training arguments.")
    training_args = TrainingArguments(
        output_dir=config.FINAL_MODEL_DIR + "_checkpoints",
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=16,
        num_train_epochs=4,
        weight_decay=0.01,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        save_total_limit=2,
        report_to="none",
        fp16=config.DEVICE.type == 'cuda',
        use_mps_device=config.DEVICE.type == 'mps'
    )

    # Data collator
    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_CHECKPOINT)
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # Initialize Trainer (using our custom trainer for class weights)
    logging.info("Initializing CustomTrainer.")
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    # Train
    logging.info("Starting model fine-tuning...")
    trainer.train()
    logging.info("Training complete!")

    # Evaluate on test set
    logging.info("Evaluating on the test set...")
    test_results = trainer.evaluate(eval_dataset=tokenized_datasets["test"])
    logging.info(f"Test Set Results: {test_results}")

    # Save the final model and tokenizer
    logging.info(f"Saving final best model to: {config.FINAL_MODEL_DIR}")
    trainer.save_model(config.FINAL_MODEL_DIR)
    tokenizer.save_pretrained(config.FINAL_MODEL_DIR)

    print("\n--- SUCCESS: Model fine-tuning is complete! ---")
    print(f"--- Final model saved in '{config.FINAL_MODEL_DIR}' directory. ---")

if __name__ == '__main__':
    main()